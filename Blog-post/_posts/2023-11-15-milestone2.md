---
layout: post
title:  "IFT 6758 Project: Milestone 2"
subtitle: "Six Subtitle"
date:   2023-11-11
---


## 1. Feature Engineering I


## 2. Baseline Models

In this section, we evaluate the performance of baseline logistic regression models trained on different features of our dataset. The models were trained using only the 'distance' feature, only the 'angle' feature, and both 'distance' and 'angle' features combined.

### Baseline Model Trained on Distance

![Confusion Matrix](/assets/images/conf_matrix_distance.png)
The baseline logistic regression model using only the 'distance' feature showed an accuracy of 0.9086. However, the confusion matrix:

indicates that the model predicted no positive classes, which suggests a potential issue with class imbalance or the feature's predictive capability.

### Baseline Models Trained on Angle and Both Features

Subsequent models trained on the 'angle' feature and both 'distance' and 'angle' features did not significantly improve the predictive performance, indicating that these features alone might not be sufficient for the task at hand.

### Visualization and Interpretation

The performance of these baseline models can be visually interpreted through the following plots:

![ROC Curve Comparison](/assets/images/roc_curve_plot.png)

The ROC curve comparison illustrates that while there is some ability to distinguish between the classes, the discriminative performance of the feature of distance is greater than that of the angle feature, and has nearly the same results obtained by using both feature

![Goal Rate vs Probability Percentile Plot](/assets/images/goal_rate_plot.png)

The plot depicting the goal rate against the probability percentile conveys a consistent pattern: higher probabilities predicted by the model correlate with an increased likelihood of actual goal occurrences. This reinforces the earlier inference where the 'distance' attribute outperforms the 'angle' attribute, and has a nearly identical performance as using the combination of both features.

![Cumulative Goals Plot](/assets/images/cumulative_goal_rate_plot.png)

The graph detailing the cumulative proportion of goals underscores a recurring observation: the predictive strength of the angle feature is overshadowed by that of the distance feature. Yet again, amalgamating both features delivers a nearly identical level of discrimination.

![Calibration Curve Plot](/assets/images/calibration_curve_plot.png)

Calibration curves serve as a metric to gauge the precision of a classifier's probability estimations. A visual inspection of these curves indicates a general trend of suboptimal calibration across the models, likely attributable to data imbalance. The angle feature, when isolated, exhibits modest reliability, whereas its combination with the distance feature enhances dependability. This plot also validates the positive correlation between the distance feature and the model's scoring accuracy.

To encapsulate, the distance attribute emerges as a pivotal factor for the model's efficiency. When paired with the angle attribute, the model's efficacy is modestly amplified. In contrast, the angle attribute, on its own, exhibits a tendency to align with the random baseline rather than displaying substantial discriminative capacity.

The three models can be found under the 'models' directory within the specific Comet.ml project entry.

- **Angle Model**: The logistic regression model trained exclusively on the angle feature.
- **Distance Model**: The logistic regression model trained solely on the distance feature.
- **Combined Model**: The logistic regression model that utilizes both distance and angle features for prediction.

The models are accessible through the following Comet.ml project entry:
[Comet.ml Project: NHL Data Science](https://www.comet.com/mahmoodhegazy/nhl-data-science/916f8c382bc04495adeeef4cf15b98e9?experiment-tab=assetStorage)
### Discussion

The baseline logistic regression models provided a starting point for understanding the predictive capability of individual features. However, their performance indicates the need for more complex models or additional features to capture the nuances of the dataset effectively. Future steps would involve exploring more sophisticated models or feature engineering techniques to improve the predictive performance.



## 3. Feature Engineering II

<b> List of Added Features</b><br>

<ol>
    <li> <b>gameID_eventID :</b> Event_Game ID for identifying unique events (PK)</li>
    <li> <b>game_period :</b> current period of the game.</li>
    <li> <b>game_seconds :</b> Total number of seconds elapsed in the game</li>
    <li> <b>x_coordinate :</b> shot x coordinate</li>
    <li> <b>y_coordinate :</b> shot y coordinate</li>
    <li> <b>shot_distance_to_goal :</b> distance between x,y coordinates of the shot and the net</li>
    <li> <b>shot_angle :</b> the shot angle.</li>
    <li> <b>shot_type :</b> type of shot</li>
    <li> <b>last_event :</b>  last event type</li>
    <li> <b>last_x_coordinate :</b> last event x coordinate</li>
    <li> <b>last_y_coordinate :</b> last event y coordinate</li>
    <li> <b>distance_from_last_event :</b> distance between x,y coordinates of current event and last event</li>
    <li> <b>time_from_last_event :</b> time passed in seconds since the last event</li>
    <li> <b>rebound :</b> Indicator of whether or not this is a rebound shot. Will be True on the 2nd successive shot, otherwise False.</li>
    <li> <b>speed :</b>  the distance from the previous event divided by the time since the previous event (distance_from_last_event/time_from_last_event).</li>
    <li> <b>change_in_shot_angle :</b> Represents the difference in the angles of consecutive shots relative to the goal. Specifically, it measures the deviation between the angle formed by the last shot and the goal, and the angle formed by the current rebound shot and the goal </li>
    <li> <b>time_since_powerplay_started :</b>  indicates the number of seconds that have passed since the beginning of a power-play. This timer resets to zero once the power-play concludes. </li>
    <li> <b>num_friendly_non_goalie_skaters :</b> denotes the count of skaters from the shooter's team present on the ice, excluding the goalie</li>
    <li> <b>num_opposing_non_goalie_skaters :</b> denotes the count of skaters from the opposing team present on the ice, excluding the goalie</li>
 
</ol>

<b>Comet.ml link storing the required experiment dataframe (wpg_v_wsh_2017021065):</b><br>

<a href="https://www.comet.com/mahmoodhegazy/nhl-data-science/9d2048b8c5d8422db2a2e4ee3551d5c9"> https://www.comet.com/mahmoodhegazy/nhl-data-science/9d2048b8c5d8422db2a2e4ee3551d5c9 </a>

## 4. Advanced Models


## 5. Other/Best Shot Models
<b> (ROC/AUC curve, goal rate vs probability percentile, cumulative proportion of goals vs probability percentile, and the reliability curve)</b>


### Experiments Setup (Same accross all experiments)

<b>Training Data</b> - All data for <b>regular</b> seasons 2016/17 - 2018/19.<br><br>

All Experiments have a pipeline created through the <b>create_pipeline</b> function found in other_ml_models.py. It is a versatile tool for building a complete machine learning pipeline, handling preprocessing (both numerical and categorical), optional feature selection, and model fitting in a streamlined and efficient manner. This setup is especially useful for ensuring consistency in data transformations across both training and testing datasets. The breakdown is as follows:

##### **Setting Up Encoders Choice**
- Initializes a dictionary `encoders` with `OrdinalEncoder` (default) and `OneHotEncoder`. 

##### **Creating Transformers**
- `numeric_transformer`: Processes numerical data by imputing missing values and scaling.
- `categorical_transformer`: Processes categorical data by imputing missing values and applying the chosen encoder.

##### **Column Transformer**
- `preprocessor`: A `ColumnTransformer` that applies the correct transformations to numerical and categorical columns.

##### **Building the Full Pipeline**
- Constructs a `Pipeline` object with the preprocessing steps, optional feature selection, and the classifier.

### Hyperparameter tunning Methodology
- Sci-kit learn's `GridSearchCV`  was used to perform Grid Search with 3-fold Cross-Validation across a pred-defined hyperparameter space (defined according to model). The method searchs through the specified subset of hyperparameters for a model and finds the most effective combination. The best model is then used for training and validation.

### Feature Selection Methodology
- Sci-kit learn's `RFECV` (Recursive Feature Elimination with Cross-Validation) was deployed on all algorithms to find the best features per model. It is a feature selection method that iteratively fits a model and removes the least important features based on certain criteria, using cross-validation to assess model performance at each step. This process continues until the optimal number of features is reached, ensuring that the selected features contribute most effectively to the model's predictive accuracy.

### Various Techniques Discussion

#### Random Forest 
Random Forest Classifiers are an ensemble learning method that operate by constructing a multitude of decision trees (weak learners) at training time, leading to improved accuracy and robustness to overfitting. They make predictions taking the majority vote across these diverse trees. It performed very well on the validation set with a **94.69%** accuracy. It was the 2'nd best performer among all classifiers. This was expected as Random Forests are very good at capturing non-linear relationships between features, which is often the case in complex scenarios like sports events where the outcome (goal or no goal) can depend on a multitude of interrelated factors.

##### **Experiment Link**

<a href="https://www.comet.com/mahmoodhegazy/nhl-data-science/34144ddb714e4845be497ce0e144f098">Random_Forest_Best_Shot</a><br>

##### **Pipeline**

![image](/assets/images/rf_pipe.png)

##### **Feature Selection and Importance** : 

![image](/assets/images/rf_best_features.png)

##### **ROC/AUC curve**

![image](/assets/images/rf_roc.png)

##### **Precesion-Recall curve**

![image](/assets/images/rf_prec_rec.png)

##### **Goal Rate vs probability percentile plot**

![image](/assets/images/rf_goal_rate.png)

##### **Cumulative proportion of goals vs probability percentile plot**

![image](/assets/images/rf_cumulative_goal.png)

##### **Reliability Curve**

![image](/assets/images/rf_calib.png)


#### CatBoost
With some intuition from Part 5 that Boosting algoriths might work well in this task, another high-performance open source library for gradient boosting on decision trees, CatBoost, was leveraged. CatBoost is particularly effective at processing categorical data. And since from our RF experiment we know that both `shot_type` and `last_event` (our only 2 categorical features) were in the top 4 important features, we thought to try CatBoost out. It performed the **best** amongst all models with a validation accuracy of **94.78%**.

##### **Experiment Link**

<a href="https://www.comet.com/mahmoodhegazy/nhl-data-science/5099680072f7468a9e7d2cec05526378">CatBoost_Best_Shot</a><br>

##### **Pipeline**

![image](/assets/images/cat_pipe.png)

##### **Feature Selection and Importance** : 

![image](/assets/images/cat_best_features.png)

##### **ROC/AUC curve**

![image](/assets/images/cat_roc.png)

##### **Precesion-Recall curve**

![image](/assets/images/cat_prec_rec.png)

##### **Goal Rate vs probability percentile plot**

![image](/assets/images/cat_goal_rate.png)

##### **Cumulative proportion of goals vs probability percentile plot**

![image](/assets/images/cat_cumulative_goal.png)

##### **Reliability Curve**

![image](/assets/images/cat_calib.png)



#### K-Nearest-Neighbours

K-Nearest Neighbors (KNN) is a supervised learning algorithm that classifies or predicts the group of a data point based on the proximity to its nearest neighbors in the dataset, operating without the need for predefined parameters. It performed well but was the weakest amongst the the afore-mentioned (part 6) models.

##### **Experiment Link**

<a href="https://www.comet.com/mahmoodhegazy/nhl-data-science/b80593d676374abb90be80dd5f748c5a">KNN_Best_Shot</a><br>

##### **Pipeline**

![image](/assets/images/knn_pipe.png)

##### **ROC/AUC curve**

![image](/assets/images/knn_roc.png)

##### **Precesion-Recall curve**

![image](/assets/images/knn_prec_rec.png)

##### **Goal Rate vs probability percentile plot**

![image](/assets/images/knn_goal_rate.png)

##### **Cumulative proportion of goals vs probability percentile plot**

![image](/assets/images/knn_cumulative_goal.png)

##### **Reliability Curve**

![image](/assets/images/knn_calib.png)



#### MLP (Multi-layer Perceptron classifier)
The Multilayer Perceptron (MLP) is a type of artificial neural network that operates in a feedforward manner, effectively mapping input data to corresponding outputs. It is composed of multiple layers, with each layer being fully connected to the subsequent one. It performed well but was the weakest amongst all (part 6) models.

##### **Experiment Link**

<a href="https://www.comet.com/mahmoodhegazy/nhl-data-science/713fc623dc8d4c5eb8e54a503cb0638c">MLP_Best_Shot</a><br>

##### **Pipeline**

![image](/assets/images/mlp_pipeline_arch.png)

##### **Precesion-Recall curve**

![image](/assets/images/mlp_prec_rec.png)

##### **ROC/AUC curve**

![image](/assets/images/mlp_roc.png)

##### **Goal Rate vs probability percentile plot**

![image](/assets/images/mlp_goal_rate.png)

##### **Cumulative proportion of goals vs probability percentile plot**

![image](/assets/images/mlp_cumulative_goal.png)

##### **Reliability Curve**

![image](/assets/images/mlp_calib.png)


### Conclusion

Picking the best model is not only looking at accuracy. We have a class imbalance where only ~10% of events are goals and therefore looking into other metrics such as precision and recall is crucial. 
 - Precision measures the accuracy of the positive predictions. In the context of our task, a high precision would mean that when our model predicts a goal, it's very likely to be a goal. This is important in scenarios where the credibility of the prediction is crucial.
 - Recall measures the ability of the model to find all relevant instances og goals. In the context of our task, a high recall would mean that the model is good at identifying shots that are likely to be goals. Favoring recall is ideal when we want to ensure you capture as many potential goal-scoring opportunities as possible, even if it means including more false positives.

 It is important to hit nice balance between precision and recall and the precision-recall curve with the highest AUC will be the best that strikes that balance. For our case that is **CatBoost**, which also happens to have the highest accuracy as well.





## 6. Evaluate on test set

## Test on 2019/20 Regular Season Dataset

Our evaluation of the five models on the untouched 2019/20 2020/21 regular season dataset yielded insightful results. While the logistic regression models showed consistent accuracy, they lacked in F1 score, recall, and precision, indicating that they may not be as effective in classifying goals within the dataset. On the other hand, the XGBoost and Cat Boost models not only showed higher accuracy but also performed significantly better across all other metrics, suggesting a more balanced classification capability.

### ROC/AUC Curves
![ROC Curves for All Models - Regular Season](/assets/images/roc_all_models.png)

### Goal Rate vs Probability Percentile
![Goal Rate vs Probability Percentile - Regular Season](/assets/images/all_models_goal_rate.png)

### Cumulative Proportion of Goals vs Probability Percentile
![Cumulative Proportion of Goals - Regular Season](/assets/images/all_models_cumulative_goals.png)

### Reliability Curve
![Combined Calibration Curves - Regular Season](/assets/images/combined_calibration_curves.png)

The models largely mirrored their validation set performance when applied to the untouched test set, maintaining satisfactory levels of accuracy and demonstrating reliable predictive capabilities. This consistency between the validation and test sets reaffirms the robustness of the modeling approach
## Test on 2019/20 Playoff Games

### ROC/AUC Curves
![ROC Curves for All Models - Playoff Games](/assets/images/roc_all_models_playoff.png)

### Goal Rate vs Probability Percentile
![Goal Rate vs Probability Percentile - Playoff Games](/assets/images/all_models_goal_rate_playoff.png)

### Cumulative Proportion of Goals vs Probability Percentile
![Cumulative Proportion of Goals - Playoff Games](/assets/images/all_models_cumulative_goals_playoff.png)

### Reliability Curve
![Combined Calibration Curves - Playoff Games](/assets/images/combined_calibration_curves_playoff.png)

### Summary Metrics

The table below summarizes the performance metrics of each model across both regular and playoff datasets.

| Model | Accuracy (Regular) | F1 Score (Regular) | Recall (Regular) | Precision (Regular) | Accuracy (Playoff) | F1 Score (Playoff) | Recall (Playoff) | Precision (Playoff) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Logistic Regression Distance | 0.9030 | 0.0000 | 0.0000 | 0.0000 | 0.9123 | 0.0000 | 0.0000 | 0.0000 |
| Logistic Regression Angle | 0.9030 | 0.0000 | 0.0000 | 0.0000 | 0.9123 | 0.0000 | 0.0000 | 0.0000 |
| Logistic Regression Both | 0.9030 | 0.0000 | 0.0000 | 0.0000 | 0.9123 | 0.0000 | 0.0000 | 0.0000 |
| XGBoost | 0.9401 | 0.5653 | 0.4014 | 0.9557 | 0.9469 | 0.5779 | 0.4145 | 0.9539 |
| Cat Boost | 0.9440 | 0.5972 | 0.4280 | 0.9873 | 0.9512 | 0.6172 | 0.4484 | 0.9898 |

During the playoff games, our models displayed robust generalization, with the XGBoost and Cat Boost models showing improved precision and recall, suggesting their effectiveness in capturing the dynamic nature of playoff hockey. Despite high accuracy, logistic regression models continued to struggle with positive class predictions, indicating a need for more complex models for goal event prediction. The enhanced performance of complex models in playoffs, as seen in ROC/AUC and calibration curves, underscores their reliability in high-stakes scenarios, emphasizing their predictive strength over simpler models